---
title: "Homework 5"
author: "Hanchuan Chen"
date: "2024-11-09"
output: github_document
---

```{r setup, message=FALSE}
library(tidyverse)
```

### Problem 2

#### Generate 5000 datasets from the model by setting mu = 0
```{r t_test_0}
# t test for mu=0
simulate_test = function(n=30, mu = 0, sigma=5, alpha=0.05) {
  
  results = data.frame(mu = mu, mu_hat = numeric(5000), p_value = numeric(5000))
  
  set.seed(42)
  for (i in 1:5000) {
    data = rnorm(n=n, mean = mu, sd = sigma)
    t_test = t.test(data, mu=0)
    
    tidy_test = broom::tidy(t_test)
    results$mu_hat[i] = tidy_test$estimate
    results$p_value[i] = tidy_test$p.value
  }
  
  return(results)
}

simulate_test(mu = 0) |> 
  summary() |> 
  knitr::kable()
```

#### Repeat process by setting mu range from 1 to 6
```{r t_test}
# repeat test for mu = 1 to 6
results = data.frame()
for (j in 1:6) {
  sub_results = simulate_test(mu = j)
  results = bind_rows(results, sub_results)
}
```

#### Draw the plot of the power and the true value mu.
```{r plot_diagrams}
#calculate power
rejected_p_value = 
  results |> 
  filter(p_value < 0.05) |> 
  group_by(mu) |> 
  summarize(count = n()) |> 
  mutate(prop = count / 5000)

ggplot(rejected_p_value, aes(x = mu, y = prop)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:6) +
  labs(x = "True Value of μ", 
       y = "Proportion of Rejections (power of t-test)", 
       title = "Power of the Test vs True μ") +
  theme_minimal()
```

#### Comment for first plot
From the plot above, it is clear to see that as the true μ gets larger (moving from 1 to 6), the power of t-test also increases and finally reach to 100%, making it easier to reject the null hypothesis with higher power. So true value of mu has positve association with power of t-test

#### Plot of the average estimate of 𝜇̂ and the true value of 𝜇.

```{r second_plot}
# plot the second plot
avg_all_estimate = 
  results |> 
  group_by(mu) |> 
  summarize(mean_mu_all = mean(mu_hat))

avg_rejected_estimate = 
  results |> 
  filter(p_value < 0.05) |> 
  group_by(mu) |> 
  summarize(mean_mu_rejected = mean(mu_hat))

avg_estimate = inner_join(avg_all_estimate, avg_rejected_estimate, by = "mu")

ggplot(avg_estimate, aes(x = mu)) +
  geom_line(aes(y=mean_mu_all, color = "All Samples")) +
  geom_point(aes(y=mean_mu_all, color = "All Samples")) +
  geom_line(aes(y=mean_mu_rejected, color = "Rejected Samples")) +
  geom_point(aes(y=mean_mu_rejected, color = "Rejected Samples")) +
  scale_x_continuous(breaks = 1:6) +
  labs(x = "True Value of μ", y = "Average estimate of rejected μ", 
       title = "Rejected estimate μ vs True μ") +
  scale_color_manual(values = c("All Samples" = "blue", "Rejected Samples" = "red")) +
  theme_minimal()
```

In this plot, blue line represents the average estimate of mean and red line represents the average estimate of mean only in rejected samples. For the tests with true value of μ are 1,2, and 3, the sample average of μ across tests for which the null is rejected is greater than true μ; however, they are approximately equal to true μ when the value is 4,5, and 6. 

Sample average of mu across tests for which the null is rejected does not equal to the true value of mu. This is because we are selecting cases where is likely farther from zero when we only pick rejected samples. This effect causes the average among rejected samples to be larger than the true μ, particularly for smaller true values.

### Problem 3

#### Dataset description
```{r}
#load homicide data and tidy it
homicide_df = 
  read_csv("./data/homicide-data.csv") |> 
  janitor::clean_names() |> 
  mutate(city_state = paste(city, state, sep=", ")) 
```

For the raw homicide data,there are `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns. The dataset contains detailed information about homicide cases, including demographic details of the victim and geographic details of the incident.

* uid: Unique identifier for each homicide case.
* reported_date: The date when the homicide was reported (likely in YYYYMMDD format).
* victim_last: Last name of the victim.
* victim_first: First name of the victim.
* victim_race: Race of the victim (e.g., Hispanic, White).
* victim_age: Age of the victim at the time of the incident.
* victim_sex: Sex of the victim (e.g., Male, Female).
* city: City where the homicide occurred (e.g., Albuquerque).
* state: State where the homicide occurred (e.g., NM for New Mexico).
* lat and lon: Latitude and longitude coordinates indicating the location of the incident.
* disposition: Status or outcome of the case (e.g., "Closed without arrest", "Closed by arrest").

#### Total number of homicides and the number of unsolved homicides in city
##### Table for this question is combined with confidence interval down below
```{r message=FALSE}
homicide_count = 
  homicide_df |> 
  group_by(city, disposition) |> 
  summarize(count = n()) |> 
  pivot_wider(
    names_from = disposition,
    values_from = count
  ) |> 
  janitor::clean_names() |> 
  mutate(total = closed_by_arrest + coalesce(closed_without_arrest,0) + open_no_arrest) |> 
  mutate(unsolved = coalesce(closed_without_arrest,0) + open_no_arrest) |> 
  select(city, total, unsolved)
```

#### Estimated proportion of homicides that are unsolved in Baltimore;
```{r}
baltimore_data = 
  homicide_count |> 
  filter(city == "Baltimore")

prop_test = prop.test(baltimore_data$unsolved, baltimore_data$total)
tidy_test = broom::tidy(prop_test)
cat("estimate proportion:", tidy_test$estimate)
cat("confidence interval:", tidy_test$conf.low, tidy_test$conf.high)
```

The estimated proportion of unsolved homicide cases in Baltimore, MD, is approximately 64.6%. The 95% confidence interval for this estimate ranges from about 62.8% to 66.3%.


#### Estimate and confidence interval for all cities
```{r}
calculate_prop = function(unsolved, total) {
  prop_test = prop.test(unsolved, total)
  tidy_test = broom::tidy(prop_test)
  
  list(
    estimated_proportion = tidy_test$estimate,
    conf_low = tidy_test$conf.low,
    conf_high = tidy_test$conf.high
  )
}

city_proportions = 
  homicide_count |> 
  mutate(results = map2(unsolved, total, calculate_prop)) |> 
  unnest_wider(results)

knitr::kable(city_proportions, digits = 3)
```

#### Plot showing the confidence interval of proportion of homicide in each city:
```{r}
city_proportions =
  city_proportions |> 
  arrange(desc(estimated_proportion))

city_proportions$city = 
  factor(city_proportions$city, levels = city_proportions$city)

# Plot the estimates and confidence intervals
ggplot(city_proportions, aes(x = city, y = estimated_proportion)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high), width = 0.2) +
  coord_flip() + 
  labs(
    title = "Proportion of Unsolved Homicides by City with Confidence Intervals",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme_minimal(base_size = 12)
```
























