---
title: "Homework 5"
author: "Hanchuan Chen"
date: "2024-11-09"
output: github_document
---

```{r setup, message=FALSE}
library(tidyverse)
```

### Problem 2

#### Generate 5000 datasets from the model by setting mu = 0
```{r t_test_0}
# t test for mu=0
simulate_test = function(n=30, mu = 0, sigma=5, alpha=0.05) {
  
  results = data.frame(mu = mu, mu_hat = numeric(5000), p_value = numeric(5000))
  
  set.seed(42)
  for (i in 1:5000) {
    data = rnorm(n=n, mean = mu, sd = sigma)
    t_test = t.test(data, mu=0)
    
    tidy_test = broom::tidy(t_test)
    results$mu_hat[i] = tidy_test$estimate
    results$p_value[i] = tidy_test$p.value
  }
  
  return(results)
}

simulate_test(mu = 0) |> 
  summary() |> 
  knitr::kable()
```

#### Repeat process by setting mu range from 1 to 6
```{r t_test}
# repeat test for mu = 1 to 6
results = data.frame()
for (j in 1:6) {
  sub_results = simulate_test(mu = j)
  results = bind_rows(results, sub_results)
}
```

#### Draw the plot of the power and the true value mu.
```{r plot_diagrams}
#calculate power
rejected_p_value = 
  results |> 
  filter(p_value < 0.05) |> 
  group_by(mu) |> 
  summarize(count = n()) |> 
  mutate(prop = count / 5000)

ggplot(rejected_p_value, aes(x = mu, y = prop)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:6) +
  labs(x = "True Value of Î¼", 
       y = "Proportion of Rejections (power of t-test)", 
       title = "Power of the Test vs True Î¼") +
  theme_minimal()
```

#### Comment for first plot
From the plot above, it is clear to see that as the true Î¼ gets larger (moving from 1 to 6), the power of t-test also increases and finally reach to 100%, making it easier to reject the null hypothesis with higher power. So true value of mu has positve association with power of t-test

#### Plot of the average estimate of ðœ‡Ì‚ and the true value of ðœ‡.

```{r second_plot}
# plot the second plot
avg_all_estimate = 
  results |> 
  group_by(mu) |> 
  summarize(mean_mu_all = mean(mu_hat))

avg_rejected_estimate = 
  results |> 
  filter(p_value < 0.05) |> 
  group_by(mu) |> 
  summarize(mean_mu_rejected = mean(mu_hat))

avg_estimate = inner_join(avg_all_estimate, avg_rejected_estimate, by = "mu")

ggplot(avg_estimate, aes(x = mu)) +
  geom_line(aes(y=mean_mu_all, color = "All Samples")) +
  geom_point(aes(y=mean_mu_all, color = "All Samples")) +
  geom_line(aes(y=mean_mu_rejected, color = "Rejected Samples")) +
  geom_point(aes(y=mean_mu_rejected, color = "Rejected Samples")) +
  scale_x_continuous(breaks = 1:6) +
  labs(x = "True Value of Î¼", y = "Average estimate of rejected Î¼", 
       title = "Rejected estimate Î¼ vs True Î¼") +
  scale_color_manual(values = c("All Samples" = "blue", "Rejected Samples" = "red")) +
  theme_minimal()
```

In this plot, blue line represents the average estimate of mean and red line represents the average estimate of mean only in rejected samples. For the tests with true value of Î¼ are 1,2, and 3, the sample average of Î¼ across tests for which the null is rejected is greater than true Î¼; however, they are approximately equal to true Î¼ when the value is 4,5, and 6. 

Sample average of mu across tests for which the null is rejected does not equal to the true value of mu. This is because we are selecting cases where is likely farther from zero when we only pick rejected samples. This effect causes the average among rejected samples to be larger than the true Î¼, particularly for smaller true values.

### Problem 3

#### Dataset description
```{r}
#load homicide data and tidy it
homicide_df = 
  read_csv("./data/homicide-data.csv") |> 
  janitor::clean_names() |> 
  mutate(city_state = paste(city, state, sep=", ")) 
```

For the raw homicide data,there are `r nrow(homicide_df)` rows and `r ncol(homicide_df)` columns. The dataset contains detailed information about homicide cases, including demographic details of the victim and geographic details of the incident.

* uid: Unique identifier for each homicide case.
* reported_date: The date when the homicide was reported (likely in YYYYMMDD format).
* victim_last: Last name of the victim.
* victim_first: First name of the victim.
* victim_race: Race of the victim (e.g., Hispanic, White).
* victim_age: Age of the victim at the time of the incident.
* victim_sex: Sex of the victim (e.g., Male, Female).
* city: City where the homicide occurred (e.g., Albuquerque).
* state: State where the homicide occurred (e.g., NM for New Mexico).
* lat and lon: Latitude and longitude coordinates indicating the location of the incident.
* disposition: Status or outcome of the case (e.g., "Closed without arrest", "Closed by arrest").

#### Total number of homicides and the number of unsolved homicides in city
##### Table for this question is combined with confidence interval down below
```{r message=FALSE}
homicide_count = 
  homicide_df |> 
  group_by(city, disposition) |> 
  summarize(count = n()) |> 
  pivot_wider(
    names_from = disposition,
    values_from = count
  ) |> 
  janitor::clean_names() |> 
  mutate(total = closed_by_arrest + coalesce(closed_without_arrest,0) + open_no_arrest) |> 
  mutate(unsolved = coalesce(closed_without_arrest,0) + open_no_arrest) |> 
  select(city, total, unsolved)
```

#### Estimated proportion of homicides that are unsolved in Baltimore;
```{r}
baltimore_data = 
  homicide_count |> 
  filter(city == "Baltimore")

prop_test = prop.test(baltimore_data$unsolved, baltimore_data$total)
tidy_test = broom::tidy(prop_test)
cat("estimate proportion:", tidy_test$estimate)
cat("confidence interval:", tidy_test$conf.low, tidy_test$conf.high)
```

The estimated proportion of unsolved homicide cases in Baltimore, MD, is approximately 64.6%. The 95% confidence interval for this estimate ranges from about 62.8% to 66.3%.


#### Estimate and confidence interval for all cities
```{r}
calculate_prop = function(unsolved, total) {
  prop_test = prop.test(unsolved, total)
  tidy_test = broom::tidy(prop_test)
  
  list(
    estimated_proportion = tidy_test$estimate,
    conf_low = tidy_test$conf.low,
    conf_high = tidy_test$conf.high
  )
}

city_proportions = 
  homicide_count |> 
  mutate(results = map2(unsolved, total, calculate_prop)) |> 
  unnest_wider(results)

knitr::kable(city_proportions, digits = 3)
```

#### Plot showing the confidence interval of proportion of homicide in each city:
```{r}
city_proportions =
  city_proportions |> 
  arrange(desc(estimated_proportion))

city_proportions$city = 
  factor(city_proportions$city, levels = city_proportions$city)

# Plot the estimates and confidence intervals
ggplot(city_proportions, aes(x = city, y = estimated_proportion)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high), width = 0.2) +
  coord_flip() + 
  labs(
    title = "Proportion of Unsolved Homicides by City with Confidence Intervals",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme_minimal(base_size = 12)
```
























