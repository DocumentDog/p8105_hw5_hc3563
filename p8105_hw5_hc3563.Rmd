---
title: "Homework 5"
author: "Hanchuan Chen"
date: "2024-11-09"
output: github_document
---

```{r setup, message=FALSE}
library(tidyverse)
```

### Problem 2
```{r t_test_0}
# t test for mu=0
n = 30
mu = 0
sigma = 5
alpha = 0.05

results = data.frame(mu_hat = numeric(5000), p_value = numeric(5000))

set.seed(42)
for (i in 1:5000) {
  data = rnorm(n=30, mean = mu, sd = sigma)
  t_test = t.test(data, mu=0)
  
  tidy_test = broom::tidy(t_test)
  results$mu_hat[i] = tidy_test$estimate
  results$p_value[i] = tidy_test$p.value
}
  
```

```{r t_test}
# repeat test for mu = 1 to 6
n = 30
sigma = 5
alpha = 0.05

results <- data.frame(mu = numeric(), mu_hat = numeric(), p_value = numeric())

set.seed(42)
for (j in 1:6) {
  mu = j
  sub_results = data.frame(mu = j, mu_hat = numeric(5000), p_value = numeric(5000))
  
  for (i in 1:5000) {
    data = rnorm(n=30, mean = mu, sd = sigma)
    t_test = t.test(data, mu=0)
    
    tidy_test = broom::tidy(t_test)
    sub_results$mu_hat[i] = tidy_test$estimate
    sub_results$p_value[i] = tidy_test$p.value
  }
  
  results = bind_rows(results, sub_results)
}
```

```{r plot_diagrams}
#plot the first plot
rejected_p_value = 
  results |> 
  filter(p_value < 0.05) |> 
  group_by(mu) |> 
  summarize(count = n()) |> 
  mutate(prop = count / 5000)

ggplot(rejected_p_value, aes(x = mu, y = prop)) +
  geom_line() +
  geom_point() +
  scale_x_continuous(breaks = 1:6) +
  labs(x = "True Value of μ", y = "Proportion of Rejections", 
       title = "Power of the Test vs True μ") +
  theme_minimal()
```

From the plot above, it is clear to see that As the true μ gets larger (moving from 1 to 6), the effect size increases, making it easier to reject the null hypothesis with higher power.

```{r second_plot}
# plot the second plot
avg_all_estimate = 
  results |> 
  group_by(mu) |> 
  summarize(mean_mu_all = mean(mu_hat))

avg_rejected_estimate = 
  results |> 
  filter(p_value < 0.05) |> 
  group_by(mu) |> 
  summarize(mean_mu_rejected = mean(mu_hat))

avg_estimate = inner_join(avg_all_estimate, avg_rejected_estimate, by = "mu")

ggplot(avg_estimate, aes(x = mu)) +
  geom_line(aes(y=mean_mu_all, color = "All Samples")) +
  geom_point(aes(y=mean_mu_all, color = "All Samples")) +
  geom_line(aes(y=mean_mu_rejected, color = "Rejected Samples")) +
  geom_point(aes(y=mean_mu_rejected, color = "Rejected Samples")) +
  scale_x_continuous(breaks = 1:6) +
  labs(x = "True Value of μ", y = "Average estimate of rejected μ", 
       title = "Rejected estimate μ vs True μ") +
  scale_color_manual(values = c("All Samples" = "blue", "Rejected Samples" = "red")) +
  theme_minimal()
```

In this plot, blue line represents the average estimate of mean and red line represents the average estimate of mean only in rejected samples. For the tests with true value of μ are 1,2, and 3, the sample average of μ across tests for which the null is rejected is greater than true μ; however, they are approximately equal to true μ when the value is 4,5, and 6. 

This is because we are selecting cases where is likely farther from zero when we only pick rejected samples. This effect causes the average among rejected samples to be larger than the true μ, particularly for smaller true values.

### Problem 3
```{r message=FALSE}
homicide_df = 
  read_csv("./data/homicide-data.csv") |> 
  janitor::clean_names() |> 
  mutate(city_state = paste(city, state, sep=", ")) 

homicide_count = 
  homicide_df |> 
  group_by(city, disposition) |> 
  summarize(count = n()) |> 
  pivot_wider(
    names_from = disposition,
    values_from = count
  ) |> 
  janitor::clean_names() |> 
  mutate(total = closed_by_arrest + coalesce(closed_without_arrest,0) + open_no_arrest) |> 
  mutate(unsolved = coalesce(closed_without_arrest,0) + open_no_arrest) |> 
  select(city, total, unsolved)
```

For the raw homicide data, it contains specific date, basic information of victims, and the location of homicide, finally with the disposition of the issue in 50 cities in U.S.. There are `r nrow(homicide_df)` of issues and `r ncol(homicide_df)` of variables.

```{r}
baltimore_data = 
  homicide_count |> 
  filter(city == "Baltimore")

prop_test = prop.test(baltimore_data$unsolved, baltimore_data$total)
tidy_test = broom::tidy(prop_test)
cat("estimate proportion:", tidy_test$estimate)
cat("confidence interval:", tidy_test$conf.low, tidy_test$conf.high)
```

The estimated proportion of unsolved homicide cases in Baltimore, MD, is approximately 64.6%. The 95% confidence interval for this estimate ranges from about 62.8% to 66.3%.

Below dataframe is the estimate and confidence interval for all cities, including total number of homicides and unsolved number of homicides:
```{r}
calculate_prop = function(unsolved, total) {
  prop_test = prop.test(unsolved, total)
  tidy_test = broom::tidy(prop_test)
  
  list(
    estimated_proportion = tidy_test$estimate,
    conf_low = tidy_test$conf.low,
    conf_high = tidy_test$conf.high
  )
}

city_proportions = 
  homicide_count |> 
  mutate(results = map2(unsolved, total, calculate_prop)) |> 
  unnest_wider(results)

knitr::kable(city_proportions)
```

Below is the plot showing the confidence interval of proportion of homicide in each city:
```{r}
city_proportions =
  city_proportions |> 
  arrange(desc(estimated_proportion))

city_proportions$city = 
  factor(city_proportions$city, levels = city_proportions$city)

# Plot the estimates and confidence intervals
ggplot(city_proportions, aes(x = city, y = estimated_proportion)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high), width = 0.2) +
  coord_flip() + 
  labs(
    title = "Proportion of Unsolved Homicides by City with Confidence Intervals",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme_minimal()
```
























